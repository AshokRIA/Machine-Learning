{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3e35c0",
   "metadata": {},
   "source": [
    "# Machine Learning â€“ Clustering Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad085f88",
   "metadata": {},
   "source": [
    "## Q1: Difference between K-Means and Hierarchical Clustering\n",
    "\n",
    "K-Means partitions data into a fixed number of clusters by minimizing intra-cluster variance, while Hierarchical Clustering builds a tree-like structure of clusters without requiring a predefined number.\n",
    "\n",
    "K-Means is suitable for large datasets, whereas Hierarchical Clustering is useful for exploring data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db667ffb",
   "metadata": {},
   "source": [
    "## Q2: Silhouette Score\n",
    "\n",
    "The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters. It ranges from -1 to 1, where higher values indicate better-defined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938951e",
   "metadata": {},
   "source": [
    "## Q3: Core parameters of DBSCAN\n",
    "\n",
    "DBSCAN uses `eps` (neighborhood radius) and `min_samples` (minimum points to form a dense region). These parameters control cluster density and noise detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2f47c",
   "metadata": {},
   "source": [
    "## Q4: Importance of Feature Scaling in Clustering\n",
    "\n",
    "Clustering algorithms rely on distance calculations. Feature scaling ensures all features contribute equally and prevents dominance by features with larger numeric ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5875ed",
   "metadata": {},
   "source": [
    "## Q5: Elbow Method\n",
    "\n",
    "The Elbow Method plots the number of clusters against inertia. The optimal number of clusters is chosen where adding more clusters yields diminishing returns in variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff98f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ee7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "wine = load_wine()\n",
    "X = StandardScaler().fit_transform(wine.data)\n",
    "\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Number of clusters found:\", clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93790add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "plt.scatter(X_scaled[:,0], X_scaled[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24993f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "labels = AgglomerativeClustering(n_clusters=3).fit_predict(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1939621",
   "metadata": {},
   "source": [
    "## Q10: Real-world customer segmentation workflow\n",
    "\n",
    "In an e-commerce setting, clustering can segment customers based on purchasing behavior. Data preprocessing includes handling missing values, encoding categorical variables, and scaling features. K-Means or DBSCAN can be used depending on cluster shape and noise. The number of clusters can be determined using the Elbow Method or Silhouette Score. Marketing teams benefit through targeted campaigns and personalization."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
