{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b246046b",
   "metadata": {},
   "source": [
    "# Supervised Classification — Decision Trees, SVM, and Naive Bayes\n",
    "\n",
    "**Note:** No personal data has been included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6d010",
   "metadata": {},
   "source": [
    "## Q1: What is Information Gain, and how is it used in Decision Trees?\n",
    "\n",
    "- **Information Gain (IG)** measures the reduction in entropy (uncertainty) about the target variable after splitting the data on a feature.\n",
    "- It is computed as the difference between the entropy of the parent node and the weighted sum of entropies of child nodes after the split.\n",
    "- Decision trees use Information Gain (or related metrics like Gain Ratio) to choose which feature and threshold to split on that yields the largest IG (most reduction in impurity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfd7e9",
   "metadata": {},
   "source": [
    "## Q2: What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "- **Entropy** (from information theory) measures impurity as $-\\sum p_i \\log_2 p_i$; it is sensitive to class probabilities and used with Information Gain.\n",
    "- **Gini Impurity** is $1 - \\sum p_i^2$ and measures the probability of misclassification if a label is randomly assigned according to class distribution.\n",
    "- **Differences & use-cases:** Gini is slightly faster to compute and often yields similar trees to Entropy; Entropy has stronger information-theoretic interpretation. Practically, both work well; choice may be empirical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cf037",
   "metadata": {},
   "source": [
    "## Q3: What is Pre-Pruning in Decision Trees?\n",
    "\n",
    "- **Pre-pruning** (early stopping) halts tree growth during training by applying stopping criteria (max depth, min samples per leaf, min impurity decrease, etc.).\n",
    "- Purpose: prevent overfitting by limiting complexity before the tree perfectly fits training data.\n",
    "- Trade-off: may underfit if pruning is too aggressive; selection of pre-pruning hyperparameters typically uses validation data or cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "alcohol: 0.000000\n",
      "malic_acid: 0.019574\n",
      "ash: 0.021419\n",
      "alcalinity_of_ash: 0.022219\n",
      "magnesium: 0.000000\n",
      "total_phenols: 0.000000\n",
      "flavanoids: 0.410802\n",
      "nonflavanoid_phenols: 0.000000\n",
      "proanthocyanins: 0.000000\n",
      "color_intensity: 0.403317\n",
      "hue: 0.000000\n",
      "od280/od315_of_diluted_wines: 0.022351\n",
      "proline: 0.100318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Feature importances:')\n",
    "for name, imp in zip(wine.feature_names, clf.feature_importances_):\n",
    "    print(f'{name}: {imp:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cdf76",
   "metadata": {},
   "source": [
    "## Q5: What is a Support Vector Machine (SVM)?\n",
    "\n",
    "- An SVM is a supervised learning model used for classification (and regression) that finds a hyperplane maximizing the margin between classes.\n",
    "- The support vectors are the training samples closest to the decision boundary; they define the position of the boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a362f8",
   "metadata": {},
   "source": [
    "## Q6: What is the Kernel Trick in SVM?\n",
    "\n",
    "- The kernel trick allows SVMs to operate in high-dimensional (possibly infinite-dimensional) feature spaces without explicitly computing coordinates in that space.\n",
    "- A kernel function computes dot products in the transformed feature space, enabling non-linear decision boundaries (e.g., RBF, polynomial kernels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd783912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Linear SVM): 0.955556\n",
      "Accuracy (RBF SVM): 0.711111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "svc_lin = SVC(kernel='linear', random_state=42).fit(X_train, y_train)\n",
    "svc_rbf = SVC(kernel='rbf', random_state=42).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy (Linear SVM):', accuracy_score(y_test, svc_lin.predict(X_test)))\n",
    "print('Accuracy (RBF SVM):', accuracy_score(y_test, svc_rbf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbc297",
   "metadata": {},
   "source": [
    "## Q8: What is the Naïve Bayes classifier, and why is it called 'Naïve'?\n",
    "\n",
    "- Naïve Bayes is a family of probabilistic classifiers based on Bayes' theorem, assuming feature independence given the class label.\n",
    "- It is called 'naïve' because it assumes conditional independence among features—a simplification that often works well in practice despite being unrealistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073a072",
   "metadata": {},
   "source": [
    "## Q9: Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
    "\n",
    "- **GaussianNB:** Assumes features are continuous and normally distributed within each class (useful for real-valued features).\n",
    "- **MultinomialNB:** Models feature counts (e.g., word counts) using multinomial distribution; commonly used in text classification.\n",
    "- **BernoulliNB:** Models binary/boolean features (presence/absence), suitable when features are binary indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (GaussianNB) on test data: 0.937063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "print('Accuracy (GaussianNB) on test data:', accuracy_score(y_test, gnb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897ca4",
   "metadata": {},
   "source": [
    "**End of notebook.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
